---
output:
  word_document: default
  html_document: default
---
# Thomas Schocklin
## 6/20/2022
### Module 6: Clustering 



```{r, include = FALSE}
library(tidyverse)
library(tidymodels)
```


```{r}
trucks = read_csv("trucks.csv")
```

```{r}
str(trucks)
summary(trucks)
```

Task 1: Plot the relationship between Distance and Speeding. Describe this relationship. Does there appear
to be any natural clustering of drivers?


```{r}
ggplot(trucks, aes(Speeding,Distance)) +
  geom_point()
```

I see potential for 2 to 4 clusters. A cluster betweek the distance of 10 and 75 with a speed of 0 to 10. Another potential clusters of distance of 10 to 75 and a speed of 20 to 65. THe other two clusters show a distance of 125 to 240 with a speeding of 10 to 25. Another cluster with 125 to 240 distance and a speed of 35 to 100. 


Task 2: As we did in the second clustering example, create a new data frame called “trucks_cleaned” that
contains the scaled and centered variables. Two notes: 1) The “predictor” variables in the recipe are “Distance”
and “Speeding” and 2) There is no need to create dummy variables as there are no categorical variables in
the data.


```{r}
kmeans_recipe = recipe(~ Distance + Speeding, trucks) 

trucks_dummy = kmeans_recipe %>% 
  ##step_dummy(all_nominal(), one_hot = TRUE) %>%
  step_scale(all_numeric()) %>%
  step_center(all_numeric()) 

trucks_dummy = prep(trucks_dummy, trucks) #prepares the recipe

trucks_cleaned = bake(trucks_dummy, trucks) #applies the recipe and yields a data frame
```


```{r}
summary(trucks_cleaned)
summary(trucks)
```


```{r}
set.seed(1234)
clusts = 
  tibble(k = 1:8) %>%
  mutate(
    kclust = map(k, ~kmeans(trucks_cleaned, .x)),
    tidied = map(kclust, tidy),
    glanced = map(kclust, glance),
    augmented = map(kclust, augment, trucks_cleaned)
  )

clusts
```

```{r}
clusters = 
  clusts %>%
  unnest(cols = c(tidied))

assignments = 
  clusts %>% 
  unnest(cols = c(augmented))

clusterings = 
  clusts %>%
  unnest(cols = c(glanced))
```

```{r}
p1 = 
  ggplot(assignments, aes(x = Distance, y = Speeding)) +
  geom_point(aes(color = .cluster), alpha = 0.8) + 
  facet_wrap(~ k)
p1
```


Task 3 Use k-Means clustering with two clusters (k=2) to cluster the “trucks_cleaned” data frame. Use a
random number seed of 64. Use augment to add the resulting clusters object to the the “trucks” data frame.
Design an appropriate visualization to visualize the clusters. Comment on the clusters.


```{r}
set.seed(64)
clusters = kmeans(trucks_cleaned, 2)
```


```{r}
trucks = augment(clusters, trucks)
str(trucks)
```




```{r}

ggplot(trucks, aes(x=Speeding,y=Distance,color=factor(.cluster))) + geom_point()

```


Task 4: Create a visualization to show how the cluster appear from values of k from 1 to 8. Use a random
number seed of 412. Which value of k appears to be most appropriate for this data?

```{r}
set.seed(412)
clusters = kmeans(trucks_cleaned, 4)
```


```{r}
ggplot(clusterings, aes(k, tot.withinss)) +
  geom_line() +
  geom_point()
```



```{r}
trucks = augment(clusters, trucks)
str(trucks)
```

```{r}

ggplot(trucks, aes(x=Speeding,y=Distance,color=factor(.cluster))) + geom_point()

```

K value of 4 appears to be the best fit for this data. 


Task 5: Create a plot of k versus within cluster sum of squares. Hint: We did this in the first clustering
lecture. Which value of k appears to be best?


```{r}
set.seed(64)
clusters = kmeans(trucks_cleaned, 4)
clusters
```


k value of 4 appears to be the best when looking at cluster sum of squares by cluster. 





Task 6: Repeat Task 3 for the number of clusters that you identifed in Task 5. Use the same random number
seed as in Task 3. Don’t forget to include your visualization. Comment on the resulting clusters.


```{r}
set.seed(64)
clusters = kmeans(trucks_cleaned, 4)
```


```{r}
trucks = augment(clusters, trucks)
str(trucks)
```

```{r}

ggplot(trucks, aes(x=Speeding,y=Distance,color=factor(.cluster))) + geom_point()

```

